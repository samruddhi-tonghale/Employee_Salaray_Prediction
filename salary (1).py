#code : streamlit
# -*- coding: utf-8 -*-
"""Salary.ipdb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jFReXKguE6mZNUsXTtzf0kxKqHFWuxid
"""

#Employee#Employee Salary prediction using adult csv
#load your libraries
from base64 import encode
import encodings
from json import encoder
import pandas as pd   # Pandas is used for loading and manipulating tabular data
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import streamlit as st
import joblib
import numpy as np
from numpy.random import randint

data=pd.read_csv("adult2.csv")

data

data.shape

data.head

#counting of null value
data.isna().sum()

#To read all the data
print(data['education'].value_counts())

print(data['marital-status'].value_counts())

print(data['occupation'].value_counts())

print(data['gender'].value_counts())

print(data['hours-per-week'].value_counts())

print(data['workclass'].value_counts())

print(data['native-country'].value_counts())

# Checking the attribute and clean the data
#Change 1:....................................
# Doing dimentionality reduction for 5th-6th, 1st-4th and Preschool because it is irrelevant data
#data=data[data['education']!='Preschool']
#data=data[data['education']!='1st-4th']
#data=data[data['education']!='5th-6th']
#data=data[data['education']!='7th-8th']
#data=data[data['education']!='9th']
#data=data[data['education']!='10th']
#data=data[data['education']!='11th']
#data=data[data['education']!='12th']
#print(data['education'].value_counts())

# To replace the '?' marks with 'Others' category
data.replace({'occupation':{'?':'Others'}},inplace=True)
print(data.occupation.value_counts())

# To replace the '?' marks with 'Notlisted' category
data.replace({'workclass':{'?':'NotListed'}},inplace=True)
print(data['workclass'].value_counts())

# Doing dimentionality reduction for Without-pay and Never-worked because it is irrelevent data
# We will remove it kind of removing from the list
data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']
print(data['workclass'].value_counts())

# To replace the '?' marks with 'Others' category
#data['native-country'].replace({'?':'Others'},inplace=True)
data.replace({'native-country': {'?':'Others'}}, inplace=True)
print(data['native-country'].value_counts())

data

data.shape

# Outlier
# The box means the most important age group that is needed most useful information
# Orange line shows median
# Circles means retired persons
plt.boxplot(data['age'])
plt.show

# We would remove the retired person's data because it is irrelevant.
data=data[(data['age']<=75) & (data['age']>=17)]
plt.boxplot(data['age'])
plt.show

# Use and save separate encoders properly
gender_encoder = LabelEncoder()
data['gender'] = gender_encoder.fit_transform(data['gender'])


occupation_encoder = LabelEncoder()
data['occupation'] = occupation_encoder.fit_transform(data['occupation'])

workclass_encoder = LabelEncoder()
data['workclass'] = workclass_encoder.fit_transform(data['workclass'])

country_encoder = LabelEncoder()
data['native-country'] = country_encoder.fit_transform(data['native-country'])


# Save them for later use in Streamlit
joblib.dump(gender_encoder, 'gender_encoder.pkl')
joblib.dump(occupation_encoder, 'occupation_encoder.pkl')
joblib.dump(workclass_encoder, 'workclass_encoder.pkl')
joblib.dump(country_encoder, 'country_encoder.pkl')
#from google.colab import files

#files.download('gender_encoder.pkl')
#files.download('occupation_encoder.pkl')
#files.download('workclass_encoder.pkl')
#files.download('country_encoder.pkl')
#print("Encoders downloaded to local machine.")

data


from sklearn.model_selection import train_test_split                               # Spilt the data into training and testing
#from sklearn.metrics import accuracy_score, classification_report                  # Evaluated model performance on test data
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, OneHotEncoder                    # Scales numeric features & OneHotEncoder coverts categorical data into numeric data
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline

#data.columns = data.columns.str.strip().str.lower().str.replace('-', '_')
#Change 2:..........................................
# Step 2: Define X and y
X = data[["age","gender","educational-num","occupation","workclass","native-country","hours-per-week","experience"]]    # All columns except the income column
y = data['income']      # The target column

# 3. Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()      # Finds all categorical values
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()       # Finds all numerical value

encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False) 
joblib.dump(encoder,'encoder.pkl')
#files.download('encoder.pkl')
print("Encoder downloaded to local machine.")

# 4. Preprocess features (OneHotEncode + Scale)
# One-hot encode categorical features
      # handle_unknown='ignore' - it won't crash & sparse_output=False- return a dense matrix
X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_cols]))

# Standardize numerical features
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X[numerical_cols]))

joblib.dump(scaler,'scaler.pkl')
#files.download('scaler.pkl')
print("Scaler downloaded to local machine.")

# 5. Combine processed features
X_preprocessed = pd.concat([X_scaled.reset_index(drop=True), X_encoded.reset_index(drop=True)], axis=1)

# 6. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)

# 7. Define models
models = {
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'SVM': SVR(),
    'KNN': KNeighborsRegressor(),
    
}

# 8. Train and evaluate each model
results = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = r2_score(y_test, y_pred)
    results[name] = acc

    print(f"{name} Accuracy: {acc:.4f}")
    print(r2_score(y_test, y_pred))

plt.bar(results.keys(), results.values(), color='brown')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.show()

# Get the best model
best_model_name=max(results, key=results.get)
best_model=models[best_model_name]
print(f"Best Model: {best_model_name} with Accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model,'best_model.pkl')
print("Best model saved to 'best_model.pkl'")

#from google.colab import files

# Download the model
#files.download('best_model.pkl')
print("Model downloaded to local machine.")